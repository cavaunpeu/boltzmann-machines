{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from functools import reduce\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 3\n",
    "data = np.random.binomial(n=1, p=[1, 0, 1], size=(100, n_units))\n",
    "alpha = .25\n",
    "\n",
    "biases = np.random.randn(n_units)\n",
    "weights = np.random.randn(n_units, n_units)\n",
    "\n",
    "var_combinations = list(combinations(range(n_units), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        \n",
    "    def H(self, x):\n",
    "        h = 0\n",
    "        for i, j in var_combinations:\n",
    "            h += self.weights[i, j] * x[i] * x[j]\n",
    "        h += self.biases @ x\n",
    "        return h\n",
    "    \n",
    "    def _unnormalized_likelihood(self, x):\n",
    "        return np.exp(self.H(x))\n",
    "    \n",
    "    def marginal_likelihood(self, x):\n",
    "        unnormalized_lik = 0\n",
    "        for config in product(*[[0, 1] if el == ... else [el] for el in x]):\n",
    "            config = np.array(config)\n",
    "            unnormalized_lik += np.exp(self.H(config))\n",
    "        return unnormalized_lik\n",
    "    \n",
    "    def likelihood(self, x, log=False):\n",
    "        \"\"\"\n",
    "        Must have the dimensionality of the data observations. To marginalize, put ellipses (...)\n",
    "        in the elements over which you wish to marginalize.\n",
    "        \"\"\"\n",
    "        x = np.array(x)\n",
    "        if not n_units in x.shape and len(x.shape) in (1, 2):\n",
    "            raise('Please pass 1 or more points of `n_units` dimensions')\n",
    "           \n",
    "        # compute unnormalized likelihoods\n",
    "        multiple_samples = len(x.shape) == 2\n",
    "        if multiple_samples:\n",
    "            likelihood = [self._unnormalized_likelihood(point) for point in x]\n",
    "        else:\n",
    "            likelihood = [self._unnormalized_likelihood(x)]\n",
    "        \n",
    "        # compute partition function\n",
    "        Z = sum([self._unnormalized_likelihood(config) for config in product([0, 1], repeat=n_units)])\n",
    "        \n",
    "        if log:\n",
    "            return sum([np.log(lik) - np.log(Z) for lik in likelihood])\n",
    "        else:\n",
    "            return reduce(np.multiply, [lik / Z for lik in likelihood])\n",
    "    \n",
    "    def conditional_likelihood(x, cond: dict):\n",
    "        joint = np.array(x)\n",
    "        for index, val in cond.items():\n",
    "            if isinstance(joint[index], int):\n",
    "                raise\n",
    "            joint[index] = val\n",
    "\n",
    "        evidence = [cond.get(i, ...) for i in range(len(x))]\n",
    "\n",
    "        return self._unnormalized_likelihood(joint) / self.marginal_likelihood(evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def gibbs_sampler(weights, biases, init_sample=None, n_samples=100, burn_in=25, every_n=10) -> np.array:\n",
    "    \n",
    "    if burn_in > n_samples:\n",
    "        raise(\"Can't burn in for more samples than there are in the chain\")\n",
    "        \n",
    "    init_sample = init_sample or [0 for _ in biases]\n",
    "    samples = [init_sample]\n",
    "    \n",
    "    def _gibbs_step(sample, i):\n",
    "        z = sum([weights[i, j] * sample[j] for j in range(len(sample)) if j != i]) + biases[i]\n",
    "        p = inv_logit(z)\n",
    "        return np.random.binomial(n=1, p=p)\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        sample = list(samples[-1])  # make copy\n",
    "        for i, _ in enumerate(sample):\n",
    "            sample[i] = _gibbs_step(sample=sample, i=i)\n",
    "        samples.append( sample )\n",
    "        \n",
    "    return np.array([sample for i, sample in enumerate(samples[burn_in:]) if i % every_n == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def update_parameters(weights=weights, biases=biases):\n",
    "    model_samples = gibbs_sampler(weights=weights, biases=biases, n_samples=1000)\n",
    "\n",
    "    for i, j in var_combinations:\n",
    "        # positive phase\n",
    "        positive_phase = (data[:, i] * data[:, j]).mean()\n",
    "\n",
    "        # negative phase\n",
    "        negative_phase = (model_samples[:, i] * model_samples[:, j]).mean()\n",
    "\n",
    "        # update weights\n",
    "        weights[i, j] += alpha * (positive_phase - negative_phase)\n",
    "        \n",
    "    for i, _ in enumerate(biases):\n",
    "        # positive phase\n",
    "        positive_phase = data[:, i].mean()\n",
    "        \n",
    "        # negative phase\n",
    "        negative_phase = model_samples[:, i].mean()\n",
    "        \n",
    "        # update biases\n",
    "        biases[i] += alpha * (positive_phase - negative_phase)\n",
    "        \n",
    "    return np.array(weights), np.array(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_n_samples(n, weights=weights, biases=biases):\n",
    "    fig = plt.figure(figsize=(12, 9))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    samples = gibbs_sampler(n_samples=n, weights=weights, biases=biases)\n",
    "    x, y, z = zip(*np.array(samples))\n",
    "    \n",
    "    x += np.random.randn(len(x)) * .05\n",
    "    y += np.random.randn(len(y)) * .05\n",
    "    z += np.random.randn(len(z)) * .05\n",
    "    \n",
    "    ax.scatter(x, y, z)\n",
    "\n",
    "# print(f'sum(x): {sum(x)} | sum(y): {sum(y)} | sum(z): {sum(z)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(weights=weights, biases=biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00764502270503839"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.likelihood([[1, 0, 1], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28197655013773254"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.likelihood([0, 0, 1]) * model.likelihood([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28197655013773254"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.likelihood([0, 0, 1], .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26879859,  0.70904794, -1.15882331],\n",
       "       [-1.03399475, -1.41967822,  0.75022826],\n",
       "       [-0.16073045,  1.69462416,  0.48787895]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7770193048567346"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(model.likelihood([1, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7770193048567346"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.likelihood([1, 0, 1], log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.likelihood([1, 0, 1], log=True) == np.log(model.likelihood([1, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30242740509042776\n",
      "0.3497552080496797\n",
      "0.3910308709248392\n",
      "0.4193612233689859\n",
      "0.44717897908954407\n",
      "0.4704027907042031\n",
      "0.5038645822321107\n",
      "0.5286349810428999\n",
      "0.5530611447104644\n",
      "0.5673064018353872\n",
      "0.5822006975570396\n",
      "0.602201617223969\n",
      "0.618474468048444\n",
      "0.6390323803927941\n",
      "0.6561525266696627\n",
      "0.6649590334317392\n",
      "0.6741230423303203\n",
      "0.6839077278979246\n",
      "0.697914444827443\n",
      "0.7109174568626483\n",
      "0.7194558459653888\n",
      "0.7309047824862285\n",
      "0.7418897066746011\n",
      "0.752691432115506\n",
      "0.7591542723149209\n",
      "0.7656202770264351\n",
      "0.7716336381321918\n",
      "0.7810067502909177\n",
      "0.7855213912357475\n",
      "0.7899555120535663\n",
      "0.797641254206316\n",
      "0.8026223537118574\n",
      "0.8075219421916014\n",
      "0.812032784100748\n",
      "0.8190318001474552\n",
      "0.824243094137564\n",
      "0.8293629288693967\n",
      "0.8327452116549278\n",
      "0.834792086529024\n",
      "0.8364248802693499\n",
      "0.8387047812549784\n",
      "0.847413427693415\n",
      "0.8519419704565414\n",
      "0.8572649688516429\n",
      "0.8625088188266651\n",
      "0.8656968813370706\n",
      "0.8706557354944301\n",
      "0.8742804200392055\n",
      "0.876014213548627\n",
      "0.8783454031494586\n",
      "0.8815003465926821\n",
      "0.883175595194476\n",
      "0.8855297587056383\n",
      "0.8892271614339585\n",
      "0.8916663149470035\n",
      "0.8945324052576775\n",
      "0.8968428376135014\n",
      "0.8979647771520067\n",
      "0.9005480726163696\n",
      "0.902743447794661\n",
      "0.9036239746177609\n",
      "0.9058763982477246\n",
      "0.9080979833637932\n",
      "0.9112715557424637\n",
      "0.9129165205740797\n",
      "0.9140339436336398\n",
      "0.9158850242028046\n",
      "0.9169805611948407\n",
      "0.9182047983131341\n",
      "0.9204549244895293\n",
      "0.9211629354323306\n",
      "0.9234746286227548\n",
      "0.9246450620839025\n",
      "0.9259001890247331\n",
      "0.9269768302873981\n",
      "0.9282447084111661\n",
      "0.9290988848110463\n",
      "0.9299563350640861\n",
      "0.9308635639358942\n",
      "0.9328772764986926\n",
      "0.9340237124067171\n",
      "0.9367564317956203\n",
      "0.9380927978256203\n",
      "0.9387790306275596\n",
      "0.9403130852829729\n",
      "0.9404355122827349\n",
      "0.9414427391895591\n",
      "0.9425707724544236\n",
      "0.9439780978449278\n",
      "0.9447663406888384\n",
      "0.9459011813478567\n",
      "0.9464846187408111\n",
      "0.9467090730137947\n",
      "0.9479397707435235\n",
      "0.9492210839763573\n",
      "0.9510282927816067\n",
      "0.9512007966483494\n",
      "0.9513450823150166\n",
      "0.9519456952051498\n",
      "0.9524023621063116\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    weights, biases = update_parameters(weights=weights, biases=biases)\n",
    "    lik = Model(weights=weights, biases=biases).likelihood([1, 0, 1])\n",
    "    print(lik)\n",
    "# plot_n_samples(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(weights=weights, biases=biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990537166678065"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.likelihood([1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.48972103,  3.33757027],\n",
       "       [-0.48972103,  0.        , -0.60588939],\n",
       "       [ 3.33757027, -0.60588939,  0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_samples = gibbs_sampler(weights=weights, biases=biases, n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.01020408, 0.98979592])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_samples.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6890563397915714e-06"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood(data.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(x):\n",
    "    return x.T @ weights @ x - biases.T @ x\n",
    "\n",
    "\n",
    "# def H(x):\n",
    "#     vals = {\n",
    "#         (0, 0, 0): -1,\n",
    "#         (0, 0, 1): -3,\n",
    "#         (0, 1, 0): -3,\n",
    "#         (0, 1, 1): 5,\n",
    "#         (1, 0, 0): 3,\n",
    "#         (1, 0, 1): -3,\n",
    "#         (1, 1, 0): -1,\n",
    "#         (1, 1, 1): 1,\n",
    "#     }\n",
    "#     return vals[tuple(x)]\n",
    "\n",
    "\n",
    "def unnormalized_likelihood(x):\n",
    "    is_marginal_lik = any([el == ... for el in x])\n",
    "    if is_marginal_lik:\n",
    "        unnormalized_lik = 0\n",
    "        for config in product(*[[0, 1] if el == ... else [el] for el in x]):\n",
    "            config = np.array(config)\n",
    "            unnormalized_lik += np.exp(H(config))\n",
    "    else:\n",
    "        unnormalized_lik = np.exp(H(x))\n",
    "    return unnormalized_lik\n",
    "\n",
    "\n",
    "def likelihood(x):\n",
    "    \"\"\"\n",
    "    Must have the dimensionality of the data observations. To marginalize, put ellipses (...)\n",
    "    in the elements over which you wish to marginalize.\n",
    "    \"\"\"\n",
    "    numerator = unnormalized_likelihood(x)\n",
    "    \n",
    "    denominator = 0\n",
    "    for config in product([0, 1], repeat=len(x)):\n",
    "        config = np.array(config)\n",
    "        denominator += np.exp(H(config))\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def conditional_likelihood(x, cond: dict):\n",
    "    joint = np.array(x)\n",
    "    for index, val in cond.items():\n",
    "        if isinstance(joint[index], int):\n",
    "            raise\n",
    "        joint[index] = val\n",
    "        \n",
    "    evidence = [cond.get(i, ...) for i in range(len(x))]\n",
    "    \n",
    "    return unnormalized_likelihood(joint) / unnormalized_likelihood(evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(x_1 = 1 | x_2 = 2) = P(x_1 = 1, x_2 = 2) / P(x_2 = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13492854264366042"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, ..., ...])\n",
    "\n",
    "likelihood(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9799882683585315"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_likelihood(x, cond={2: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(x_1 = 1) = P(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets generate data\n",
    "\n",
    "n_outputs = 10\n",
    "n_points = 100\n",
    "data = np.random.binomial(n=1, p=.6, size=(n_points, n_outputs))\n",
    "weights = np.random.randn(n_points, n_points)\n",
    "np.fill_diagonal(weights, 0)\n",
    "bias = np.random.randn(n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, derivation of gradient updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
