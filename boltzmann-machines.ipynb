{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 3\n",
    "data = np.random.binomial(n=1, p=[1, 0, 1], size=(100, n_units))\n",
    "alpha = .25\n",
    "\n",
    "biases = np.random.randn(n_units)\n",
    "weights = np.random.randn(n_units, n_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def gibbs_sampler(weights, biases, init_sample=None, n_samples=100, burn_in=25, every_n=10) -> np.array:\n",
    "    \n",
    "    if burn_in > n_samples:\n",
    "        raise(\"Can't burn in for more samples than there are in the chain\")\n",
    "        \n",
    "    init_sample = init_sample or [0 for _ in biases]\n",
    "    samples = [init_sample]\n",
    "    \n",
    "    def _gibbs_step(sample, i):\n",
    "        z = sum([weights[i, j] * sample[j] for j in range(len(sample)) if j != i]) + biases[i]\n",
    "        p = inv_logit(z)\n",
    "        return np.random.binomial(n=1, p=p)\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        sample = list(samples[-1])  # make copy\n",
    "        for i, _ in enumerate(sample):\n",
    "            sample[i] = _gibbs_step(sample=sample, i=i)\n",
    "        samples.append( sample )\n",
    "        \n",
    "    return np.array([sample for i, sample in enumerate(samples[burn_in:]) if i % every_n == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_n_samples(n, weights=weights, biases=biases):\n",
    "    fig = plt.figure(figsize=(12, 9))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    samples = gibbs_sampler(n_samples=n, weights=weights, biases=biases)\n",
    "    x, y, z = zip(*np.array(samples))\n",
    "    \n",
    "    x += np.random.randn(len(x)) * .05\n",
    "    y += np.random.randn(len(y)) * .05\n",
    "    z += np.random.randn(len(z)) * .05\n",
    "    \n",
    "    ax.scatter(x, y, z)\n",
    "\n",
    "# print(f'sum(x): {sum(x)} | sum(y): {sum(y)} | sum(z): {sum(z)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "weight_combinations = list(combinations(range(n_units), 2))\n",
    "\n",
    "\n",
    "def update_parameters(weights=weights, biases=biases):\n",
    "    model_samples = gibbs_sampler(weights=weights, biases=biases, n_samples=1000)\n",
    "\n",
    "    for i, j in weight_combinations:\n",
    "        # positive phase\n",
    "        positive_phase = (data[:, i] * data[:, j]).mean()\n",
    "\n",
    "        # negative phase\n",
    "        negative_phase = (model_samples[:, i] * model_samples[:, j]).mean()\n",
    "\n",
    "        # update weights\n",
    "        weights[i, j] += alpha * (positive_phase - negative_phase)\n",
    "        \n",
    "    for i, _ in enumerate(biases):\n",
    "        # positive phase\n",
    "        positive_phase = data[:, i].mean()\n",
    "        \n",
    "        # negative phase\n",
    "        negative_phase = model_samples[:, i].mean()\n",
    "        \n",
    "        # update biases\n",
    "        biases[i] += alpha * (positive_phase - negative_phase)\n",
    "        \n",
    "    return np.array(weights), np.array(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_samples = gibbs_sampler(weights=weights, biases=biases, n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.01020408, 0.98979592])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_samples.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    weights, biases = update_parameters(weights=weights, biases=biases)\n",
    "# plot_n_samples(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6890563397915714e-06"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood(data.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(x):\n",
    "    return x.T @ weights @ x - biases.T @ x\n",
    "\n",
    "\n",
    "# def H(x):\n",
    "#     vals = {\n",
    "#         (0, 0, 0): -1,\n",
    "#         (0, 0, 1): -3,\n",
    "#         (0, 1, 0): -3,\n",
    "#         (0, 1, 1): 5,\n",
    "#         (1, 0, 0): 3,\n",
    "#         (1, 0, 1): -3,\n",
    "#         (1, 1, 0): -1,\n",
    "#         (1, 1, 1): 1,\n",
    "#     }\n",
    "#     return vals[tuple(x)]\n",
    "\n",
    "\n",
    "def unnormalized_likelihood(x):\n",
    "    is_marginal_lik = any([el == ... for el in x])\n",
    "    if is_marginal_lik:\n",
    "        unnormalized_lik = 0\n",
    "        for config in product(*[[0, 1] if el == ... else [el] for el in x]):\n",
    "            config = np.array(config)\n",
    "            unnormalized_lik += np.exp(H(config))\n",
    "    else:\n",
    "        unnormalized_lik = np.exp(H(x))\n",
    "    return unnormalized_lik\n",
    "\n",
    "\n",
    "def likelihood(x):\n",
    "    \"\"\"\n",
    "    Must have the dimensionality of the data observations. To marginalize, put ellipses (...)\n",
    "    in the elements over which you wish to marginalize.\n",
    "    \"\"\"\n",
    "    numerator = unnormalized_likelihood(x)\n",
    "    \n",
    "    denominator = 0\n",
    "    for config in product([0, 1], repeat=len(x)):\n",
    "        config = np.array(config)\n",
    "        denominator += np.exp(H(config))\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def conditional_likelihood(x, cond: dict):\n",
    "    joint = np.array(x)\n",
    "    for index, val in cond.items():\n",
    "        if isinstance(joint[index], int):\n",
    "            raise\n",
    "        joint[index] = val\n",
    "        \n",
    "    evidence = [cond.get(i, ...) for i in range(len(x))]\n",
    "    \n",
    "    return unnormalized_likelihood(joint) / unnormalized_likelihood(evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(x_1 = 1 | x_2 = 2) = P(x_1 = 1, x_2 = 2) / P(x_2 = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13492854264366042"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, ..., ...])\n",
    "\n",
    "likelihood(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9799882683585315"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_likelihood(x, cond={2: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(x_1 = 1) = P(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets generate data\n",
    "\n",
    "n_outputs = 10\n",
    "n_points = 100\n",
    "data = np.random.binomial(n=1, p=.6, size=(n_points, n_outputs))\n",
    "weights = np.random.randn(n_points, n_points)\n",
    "np.fill_diagonal(weights, 0)\n",
    "bias = np.random.randn(n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, derivation of gradient updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
